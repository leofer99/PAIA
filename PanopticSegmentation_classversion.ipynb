{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZWQ9x9e2AIy"
   },
   "source": [
    "# Panoptic Segmentation\n",
    "\n",
    "In panoptic segmentation the image is segmented into known objects (things) but also considering amorphous regions (stuff).\n",
    "\n",
    "We follow these steps:\n",
    "\n",
    "1. Load a pre-trained model\n",
    "1. Test the model in one image\n",
    "1. Show the segmentation result\n",
    "1. Use Detectron2 utils to show more information about the results\n",
    "\n",
    "Based on: https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Cw0BKL-2ZTc"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/cocodataset/panopticapi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Br9HPaV22AI1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panopticapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpanopticapi\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'panopticapi'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import math\n",
    "import torch\n",
    "import panopticapi\n",
    "import requests\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeX7NpBa2AI2"
   },
   "source": [
    "### Load a pre-trained model\n",
    "\n",
    "In this case we will be using the DETR pre-trained model. It is based on a transformer architecture (https://github.com/facebookresearch/detr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue9Ip-Mm2AI2"
   },
   "outputs": [],
   "source": [
    "model, postprocessor = torch.hub.load('facebookresearch/detr', 'detr_resnet101_panoptic', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4IPg_UQ2AI2"
   },
   "source": [
    "### Test the model in one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aTs2BKI2AI3"
   },
   "outputs": [],
   "source": [
    "#url = \"http://images.cocodataset.org/val2017/000000281759.jpg\"\n",
    "#url = \"http://images.cocodataset.org/val2017/000000289222.jpg\"\n",
    "#url = \"http://images.cocodataset.org/val2017/000000439715.jpg\"\n",
    "url = \"http://images.cocodataset.org/val2017/000000324158.jpg\"\n",
    "\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmXMe_S_2AI3"
   },
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAr-UE2a2AI3"
   },
   "outputs": [],
   "source": [
    "# mean-std normalize the input image (batch-size: 1)\n",
    "# and pass it to the model\n",
    "img = transform(im).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOpDfihf2AI3"
   },
   "outputs": [],
   "source": [
    "# let's explore what the model outputs\n",
    "print(out.keys())\n",
    "print(out['pred_logits'].shape)\n",
    "print(out['pred_boxes'].shape)\n",
    "print(out['pred_masks'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HApIU9gw2AI4"
   },
   "outputs": [],
   "source": [
    "# compute the scores, excluding the \"no-object\" class (the last one)\n",
    "scores = out['pred_logits'].softmax(dim=-1)[..., :-1].max(dim=-1)[0]\n",
    "# threshold the confidence\n",
    "keep = scores > 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YCLjxDxF2AI4"
   },
   "outputs": [],
   "source": [
    "# plot the masks\n",
    "ncols = 5\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), figsize=(18, 10))\n",
    "for line in axs:\n",
    "    for a in line:\n",
    "        a.axis('off')\n",
    "for i, mask in enumerate(out[\"pred_masks\"][keep]):\n",
    "    ax = axs[i // ncols, i % ncols]\n",
    "    ax.imshow(mask.detach().numpy(), cmap=\"cividis\")\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRwrtFSW2AI4"
   },
   "source": [
    "### Show the segmentation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0WYMFR62AI4"
   },
   "outputs": [],
   "source": [
    "# the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtTeRKor2AI5"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "# The segmentation is stored in a special-format png\n",
    "panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
    "# We retrieve the ids corresponding to each mask\n",
    "panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "\n",
    "# Finally we color each mask individually\n",
    "panoptic_seg[:, :, :] = 0\n",
    "for id in range(panoptic_seg_id.max() + 1):\n",
    "  panoptic_seg[panoptic_seg_id == id] = np.asarray(next(palette)) * 255\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(panoptic_seg)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8mP7iwz2AI5"
   },
   "source": [
    "### Use Detectron2 utils to show more information\n",
    "\n",
    "The function createResultsImage can be used to show further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZN9MPc84A-K"
   },
   "outputs": [],
   "source": [
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZywcppA2AI5"
   },
   "outputs": [],
   "source": [
    "# function to create an image with the segments overlayed in the image\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from copy import deepcopy\n",
    "def createResultsImage(im, result):\n",
    "    # We extract the segments info and the panoptic result from DETR's prediction\n",
    "    segments_info = deepcopy(result[\"segments_info\"])\n",
    "    # Panoptic predictions are stored in a special format png\n",
    "    panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "    final_w, final_h = panoptic_seg.size\n",
    "    # We convert the png into an segment id map\n",
    "    panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
    "    panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg))\n",
    "\n",
    "    # Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly\n",
    "    meta = MetadataCatalog.get(\"coco_2017_val_panoptic_separated\")\n",
    "    for i in range(len(segments_info)):\n",
    "        c = segments_info[i][\"category_id\"]\n",
    "        segments_info[i][\"category_id\"] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][\"isthing\"] else meta.stuff_dataset_id_to_contiguous_id[c]\n",
    "\n",
    "    # Finally we visualize the prediction\n",
    "    v = Visualizer(np.array(im.copy().resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0)\n",
    "    v._default_font_size = 20\n",
    "    v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0)\n",
    "\n",
    "    return Image.fromarray(v.get_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NZhiimh2AI5"
   },
   "outputs": [],
   "source": [
    "extendedResults = createResultsImage(im, result)\n",
    "extendedResults"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
