{"metadata":{"language_info":{"name":"python"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0,"cells":[{"source":["# Image Inpainting"],"cell_type":"markdown","metadata":{"id":"_jMrBoE6P2Yc"}},{"source":["## Traditional Methods\n","\n","OpenCV implements two traditional methods for inpainting:\n","\n","* cv2.INPAINT_TELEA: An image inpainting technique based on the fast marching method (Telea, 2004)\n","\n","* cv2.INPAINT_NS: Navier-stokes, Fluid dynamics, and image and video inpainting (Bertalm√≠o et al., 2001)"],"cell_type":"markdown","metadata":{"id":"AkC3zRSlP2Yj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"QNzZCvpTP2Yk","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1716218244437,"user_tz":-60,"elapsed":1064,"user":{"displayName":"Leonor Justo dos Santos Fernandes","userId":"06258688304767564933"}},"outputId":"58b665ea-c681-49c4-a755-12e1d644a286"},"outputs":[{"output_type":"error","ename":"error","evalue":"OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8148d15e4428>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# read image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat_damaged.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimg_toplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# plot image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","# read image\n","img = cv2.imread('cat_damaged.png')\n","img_toplot = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# plot image\n","plt.imshow(img_toplot)\n","plt.axis('off')\n","plt.show()\n","\n","# read mask\n","mask = cv2.imread('cat_mask.png', 0)\n","\n","# plot mask\n","plt.imshow(mask, cmap='gray')\n","plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lukF1jV8P2Yo","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"error","timestamp":1716218042360,"user_tz":-60,"elapsed":492,"user":{"displayName":"Isabel Rio-Torto","userId":"08770351900723923855"}},"outputId":"6a56f467-783c-4b48-a3e4-8a34ee6f30d6"},"outputs":[{"output_type":"error","ename":"DisabledFunctionError","evalue":"cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ea81fe5ecb3e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# inpaint using OpenCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minpaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINPAINT_NS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"],"errorDetails":{"actions":[{"action":"open_snippet","actionText":"Search Snippets for cv2.imshow","snippetFilter":"cv2.imshow"}]}}],"source":["# inpaint using OpenCV\n","dst = cv2.inpaint(img, mask, 3, cv2.INPAINT_NS)\n","cv2.imshow(dst)"]},{"source":["---\n","## Deep Learning Methods\n","\n","Generative Image Inpainting with Contextual Attention (Yu et al. 2018)\n","\n","Adapted from: https://github.com/daa233/generative-inpainting-pytorch\n"],"cell_type":"markdown","metadata":{"id":"DtXk5OaRP2Yp"}},{"cell_type":"code","source":["!unzip model.zip"],"metadata":{"id":"hAWbGt24U8sP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716218720563,"user_tz":-60,"elapsed":1160,"user":{"displayName":"Leonor Justo dos Santos Fernandes","userId":"06258688304767564933"}},"outputId":"e6b73d3e-93c4-4e30-ce73-62b3965bd33d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  model.zip\n","error [model.zip]:  missing 9943 bytes in zipfile\n","  (attempting to process anyway)\n","error: invalid zip file with overlapped components (possible zip bomb)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wTUQo3QrP2Yq","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1716218739222,"user_tz":-60,"elapsed":7519,"user":{"displayName":"Leonor Justo dos Santos Fernandes","userId":"06258688304767564933"}},"outputId":"3ce7528b-71c0-46af-981d-1aaa7cee2122"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'model'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-568a1cf8c959>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","\n","from model.networks import Generator\n","from utils.tools import normalize, get_model_list\n","\n","checkpoint_path = 'model/pretrained'\n","output_path = 'output.png'\n","image_shape = [256, 256, 3]\n","netG_config = {'input_dim': 3, 'ngf': 32}\n","cuda = False\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5npOSbRyP2Yr"},"outputs":[],"source":["x = Image.open('test.png')\n","mask = Image.open('test_mask.png')\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMElQL0zP2Ys"},"outputs":[],"source":["transform = transforms.Compose([\n","            transforms.Resize(image_shape[:-1]),\n","            transforms.CenterCrop(image_shape[:-1]),\n","            transforms.ToTensor(),\n","])\n","\n","x = transform(x)\n","mask = transform(mask)\n","mask = mask[0].unsqueeze(dim=0)\n","\n","x_show = x.clone()\n","x = normalize(x)\n","x = x * (1. - mask)\n","x = x.unsqueeze(dim=0)\n","mask = mask.unsqueeze(dim=0)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CBEpWXb9P2Yt","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1716218680061,"user_tz":-60,"elapsed":966,"user":{"displayName":"Leonor Justo dos Santos Fernandes","userId":"06258688304767564933"}},"outputId":"bb06957a-4a8f-4cd3-b2e2-72d5193903cc"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-17d9e1fd4778>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# build network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mnetG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlast_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["with torch.no_grad():\n","  # build network\n","  netG = Generator(netG_config, False, device)\n","  last_model_name = get_model_list(checkpoint_path, \"gen\")\n","  print(last_model_name)\n","\n","  # load pretrained weights\n","  ckpt = torch.load(last_model_name)\n","  netG.load_state_dict(ckpt)\n","\n","  # do inference\n","  _, x2, _ = netG(x, mask)\n","  #x2 is a tensor with shape\n","\n","  # save and visualize result\n","  #print(x2)\n","  print(x2.shape)\n","\n","  vutils.save_image(x2, 'x2.png', padding=0, nrmalize=True)\n","  im= Image.open('x2.png')\n","  im\n","\n","  #better image for sure than traditional methods, still a bit fuzzy when zooming in (dependent on amount of images the model can train on)"]}]}